apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  labels:
    prometheus: k8s
    role: alert-rules
  name: prometheus-ceph-rules
  namespace: default
spec:
  groups:
  - name: quorum-alert.rules
    rules:
    - alert: MonQuorumAtRisk
      annotations:
        description: Quorum is low for storage cluster. Please Contact Support
        message: Storage quorum at risk
        severity_level: warning
        storage_type: ceph
      expr: |
        count(ceph_mon_quorum_status == 1) <= ((count(ceph_mon_metadata) % 2) + 1)
      for: 15m
      labels:
        severity: warning
  - name: osd-alert.rules
    rules:
    - alert: CephOSDDiskNotResponding
      annotations:
        description: Disk not responding, on host {{ $labels.host }} (device {{ $labels.device
          }})
        message: Disk not responding
        severity_level: warning
        storage_type: ceph
      expr: |
        label_replace((ceph_osd_in == 1 and ceph_osd_up == 0),"disk","$1","ceph_daemon","osd.(.*)") + on(ceph_daemon) group_left(host, device) label_replace(ceph_disk_occupation,"host","$1","exported_instance","(.*)")
      for: 1m
      labels:
        severity: warning
    - alert: CephOSDDiskUnavailable
      annotations:
        description: Disk inaccessible on host {{ $labels.host }} (device {{ $labels.device
          }})
        message: Disk is inaccessible
        severity_level: warning
        storage_type: ceph
      expr: |
        label_replace((ceph_osd_in == 0 and ceph_osd_up == 0),"disk","$1","ceph_daemon","osd.(.*)") + on(ceph_daemon) group_left(host, device) label_replace(ceph_disk_occupation,"host","$1","exported_instance","(.*)")
      for: 1m
      labels:
        severity: warning
    - alert: CephDataRecoveryActive
      annotations:
        description: Data recovery is active, resynchronizing data to the required
          data protection level
        message: Data recovery is active
        severity_level: info
        storage_type: ceph
      expr: |
        rate(ceph_pg_undersized[30s]) > 0 and ceph_pg_undersized > 0
      for: 30s
      labels:
        severity: info
    - alert: CephDataRecoveryQueued
      annotations:
        description: Data recovery is queued
        message: Data recovery is queued
        severity_level: info
        storage_type: ceph
      expr: |
        rate(ceph_pg_undersized[30s]) == 0 and ceph_pg_undersized > 0
      for: 30s
      labels:
        severity: info
    - alert: CephDataRecoveryTakingTooLong
      annotations:
        description: Data recovery has been active for over 2h. Contact Support
        message: Data recovery is slow
        severity_level: warning
        storage_type: ceph
      expr: |
        ceph_pg_undersized > 0
      for: 2h
      labels:
        severity: warning
    - alert: CephDataRebalanceQueued
      annotations:
        description: Data rebalance is queued (rebalance improves disk utilization
          and performance)
        message: Data rebalance queued
        severity_level: info
        storage_type: ceph
      expr: |
        rate(ceph_pg_remapped[30s]) == 0 and ceph_pg_remapped > 0 and ceph_pg_undersized == 0
      for: 15s
      labels:
        severity: info
    - alert: CephDataRebalanceActive
      annotations:
        description: Data rebalance is active (rebalance improves disk utilization
          and performance)
        message: Data rebalance active
        severity_level: info
        storage_type: ceph
      expr: |
        rate(ceph_pg_remapped[30s]) > 0 and ceph_pg_remapped > 0 and ceph_pg_undersized == 0
      for: 15s
      labels:
        severity: info
    - alert: CephPGRepairTakingTooLong
      annotations:
        description: Self Heal operations taking too long. Contact Support
        message: Problems detected within self heal
        severity_level: warning
        storage_type: ceph
      expr: |
        ceph_pg_inconsistent > 0
      for: 1h
      labels:
        severity: warning
  - name: cluster-state-alert.rules
    rules:
    - alert: CephClusterErrorState
      annotations:
        description: Storage cluster is in error state for more than 10m.
        message: Storage cluster is in error state
        severity_level: error
        storage_type: ceph
      expr: |
        ceph_health_status{job="rook-ceph-mgr"} > 1
      for: 10m
      labels:
        severity: critical
    - alert: CephClusterWarningState
      annotations:
        description: Storage cluster is in warning state for more than 10m.
        message: Storage cluster is in warning state
        severity_level: warning
        storage_type: ceph
      expr: |
        ceph_health_status{job="rook-ceph-mgr"} == 1
      for: 10m
      labels:
        severity: warning
  - name: cluster-utilization-alert.rules
    rules:
    - alert: CephClusterNearFull
      annotations:
        description: The utilization of storage cluster has crossed 85%.
        message: Storage cluster is nearing full. An expansion is required
        severity_level: warning
        storage_type: ceph
      expr: |
        sum(ceph_osd_stat_bytes_used) / sum(ceph_osd_stat_bytes) > 0.85
      for: 5m
      labels:
        severity: warning
    - alert: CephClusterCriticallyFull
      annotations:
        description: The utilization of storage cluster has crossed 95%.
        message: Storage cluster is critically full and needs immediate expansion
        severity_level: error
        storage_type: ceph
      expr: |
        sum(ceph_osd_stat_bytes_used) / sum(ceph_osd_stat_bytes) > 0.95
      for: 5m
      labels:
        severity: critical
